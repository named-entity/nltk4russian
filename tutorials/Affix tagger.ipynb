{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Подбор оптимальных параметров тренировки аффиксного тэггера.\n",
    "\n",
    "### Гудков В.В.  1 курс магистратура СПбГУ, кафедра математической лингвистики"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk4russian.util import read_corpus_to_nltk\n",
    "from nltk import sequential\n",
    "import statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Статистика по словам\n",
    "def words_stats(sents, name = \" \"):\n",
    "    \n",
    "    words = [] #Тут будут все слова корпуса\n",
    "    \n",
    "    for sent in sents:\n",
    "        for word_pos in sent:\n",
    "            words.append(word_pos[0])\n",
    "                \n",
    "    words_lens = [len(word) for word in words] #Длины всех слов в корпусе\n",
    "    mean_words = round(statistics.mean(words_lens), 2)\n",
    "    less_than_4 = len([i for i in words_lens if i < 4])\n",
    "    more_than_8 = len([j for j in words_lens if j > 8])\n",
    "    \n",
    "    print(\"Средняя длина слова в корпусе \" + name + \" = \" + str(mean_words))\n",
    "    print(\"Количество слов меньше 4-х символов в корпусе \" + name + \" = \" + str(less_than_4) + \" (\" + \n",
    "          str(round(less_than_4 / len(words)*100, 2)) + \"%)\")\n",
    "    print(\"Количество слов больше 8-и символов в корпусе \" + name + \" = \" + str(more_than_8)+ \" (\" + \n",
    "          str(round(more_than_8/ len(words)*100, 2)) + \"%)\")\n",
    "    \n",
    "    \n",
    "#Перебор всех вариантов тэггеров для данного датасета (длины суффиксов и минимальных стемм) \n",
    "def go_thru_loop(train, test):\n",
    "    for i in range(-5,-1):\n",
    "        for j in range(4):\n",
    "            if -i+j > 6:\n",
    "                break\n",
    "            else:\n",
    "                print(\"Тренировка тэггера...\")\n",
    "                print(\"Длина суффикса \", i*-1)\n",
    "                print(\"Минимальная стемма \", j)\n",
    "                tagger = sequential.AffixTagger(train=train, affix_length=i,\n",
    "                                                          min_stem_length=j, verbose=False)\n",
    "                print(\"Результат:\", round(tagger.evaluate(test), 3)*100, \"%\")\n",
    "                print('-----------------------------')\n",
    "\n",
    "# Удаление коротких словоформ и других знаков из корпуса\n",
    "def rmv_irr_tokens(sents):\n",
    "    \n",
    "    print('--------------------')\n",
    "    print(\"Количество словоупотреблений до чистки: \" + str(sum(len(i) for i in sents)))\n",
    "    num_of_irr = 0\n",
    "    \n",
    "    for i in range(6):\n",
    "        for sent in sents:\n",
    "            for word_pos in sent:\n",
    "                pos = list(word_pos)[1].split(',')[0]\n",
    "                if pos in ['PNCT', 'NUMR', 'PRTF', 'PRCL', 'PREP', 'UNKN', 'CONJ', 'PRTS',\n",
    "                                  'sing', 'plur', 'neut', '', 'masc', 'INTJ']:\n",
    "                    sent.remove(word_pos)\n",
    "                    num_of_irr += 1\n",
    "                elif pos is 'ADJF' and len(pos) <= 4:\n",
    "                    sent.remove(word_pos)\n",
    "                    num_of_irr += 1\n",
    "            if len(sent) == 0:\n",
    "                sents.remove(sent)\n",
    "\n",
    "    print(\"Количество удаленных токенов: \" + str(num_of_irr))\n",
    "    print(\"Количество словоупотреблений после чистки: \" + str(sum(len(i) for i in sents)))\n",
    "    print('--------------------')\n",
    "    \n",
    "#Преобразование морф корпуса в POS корпус\n",
    "def full_morph_to_pos(sents):\n",
    "    \n",
    "    sent_index = 0\n",
    "    for sent in sents:\n",
    "        word_pos_index = 0\n",
    "        for word_pos in sent:\n",
    "            word_pos = list(word_pos)\n",
    "            sents[sent_index][word_pos_index] = list(sents[sent_index][word_pos_index])\n",
    "            sents[sent_index][word_pos_index][1] = word_pos[1].split(',')[0]\n",
    "            sents[sent_index][word_pos_index] = tuple(sents[sent_index][word_pos_index])\n",
    "            word_pos_index += 1\n",
    "        sent_index += 1\n",
    "        \n",
    "\n",
    "#Разметка своих слов натренированными теггерами из списка        \n",
    "def apply_tagger_to_list(tagger, word_list, counter = 0):\n",
    "\n",
    "    print(\"----------\")\n",
    "    for tok, tag in tagger.tag(word_list):\n",
    "        if tag is None:\n",
    "            counter += 1\n",
    "        print(\"(%s, %s), \" % (tok, tag))\n",
    "    print(\"----------\")\n",
    "    print(\"Неопознанных слов: %d\" % counter)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Средняя длина слова в корпусе sents_train = 5.02\n",
      "Количество слов меньше 4-х символов в корпусе sents_train = 40151 (43.3%)\n",
      "Количество слов больше 8-и символов в корпусе sents_train = 17502 (18.88%)\n",
      "--------------------------\n",
      "Средняя длина слова в корпусе sents_test = 5.12\n",
      "Количество слов меньше 4-х символов в корпусе sents_test = 8178 (42.66%)\n",
      "Количество слов больше 8-и символов в корпусе sents_test = 3804 (19.85%)\n",
      "|||||||||||||||||||||||||\n",
      "Тренировка тэггера...\n",
      "Длина суффикса  5\n",
      "Минимальная стемма  0\n",
      "Результат: 34.1 %\n",
      "-----------------------------\n",
      "Тренировка тэггера...\n",
      "Длина суффикса  5\n",
      "Минимальная стемма  1\n",
      "Результат: 28.599999999999998 %\n",
      "-----------------------------\n",
      "Тренировка тэггера...\n",
      "Длина суффикса  4\n",
      "Минимальная стемма  0\n",
      "Результат: 37.5 %\n",
      "-----------------------------\n",
      "Тренировка тэггера...\n",
      "Длина суффикса  4\n",
      "Минимальная стемма  1\n",
      "Результат: 33.5 %\n",
      "-----------------------------\n",
      "Тренировка тэггера...\n",
      "Длина суффикса  4\n",
      "Минимальная стемма  2\n",
      "Результат: 28.4 %\n",
      "-----------------------------\n",
      "Тренировка тэггера...\n",
      "Длина суффикса  3\n",
      "Минимальная стемма  0\n",
      "Результат: 38.6 %\n",
      "-----------------------------\n",
      "Тренировка тэггера...\n",
      "Длина суффикса  3\n",
      "Минимальная стемма  1\n",
      "Результат: 33.300000000000004 %\n",
      "-----------------------------\n",
      "Тренировка тэггера...\n",
      "Длина суффикса  3\n",
      "Минимальная стемма  2\n",
      "Результат: 30.2 %\n",
      "-----------------------------\n",
      "Тренировка тэггера...\n",
      "Длина суффикса  3\n",
      "Минимальная стемма  3\n",
      "Результат: 25.900000000000002 %\n",
      "-----------------------------\n",
      "Тренировка тэггера...\n",
      "Длина суффикса  2\n",
      "Минимальная стемма  0\n",
      "Результат: 33.7 %\n",
      "-----------------------------\n",
      "Тренировка тэггера...\n",
      "Длина суффикса  2\n",
      "Минимальная стемма  1\n",
      "Результат: 26.8 %\n",
      "-----------------------------\n",
      "Тренировка тэггера...\n",
      "Длина суффикса  2\n",
      "Минимальная стемма  2\n",
      "Результат: 23.799999999999997 %\n",
      "-----------------------------\n",
      "Тренировка тэггера...\n",
      "Длина суффикса  2\n",
      "Минимальная стемма  3\n",
      "Результат: 21.9 %\n",
      "-----------------------------\n"
     ]
    }
   ],
   "source": [
    "with open('media_train.tab', encoding = 'utf-8', mode='r') as f:\n",
    "    sents_train = list(read_corpus_to_nltk(f))\n",
    "\n",
    "with open('media_test.tab', encoding = 'utf-8', mode='r') as r:\n",
    "    sents_test = list(read_corpus_to_nltk(r))\n",
    "    \n",
    "words_stats(sents_train, name = \"sents_train\")\n",
    "print('--------------------------')\n",
    "words_stats(sents_test, name = \"sents_test\")\n",
    "\n",
    "print(\"|||||||||||||||||||||||||\")\n",
    "go_thru_loop(sents_train, sents_test) #Учим без предобработки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "Количество словоупотреблений до чистки: 92720\n",
      "Количество удаленных токенов: 39062\n",
      "Количество словоупотреблений после чистки: 53658\n",
      "--------------------\n",
      "--------------------\n",
      "Количество словоупотреблений до чистки: 19168\n",
      "Количество удаленных токенов: 7819\n",
      "Количество словоупотреблений после чистки: 11349\n",
      "--------------------\n",
      "Средняя длина слова в корпусе sents_train = 7.19\n",
      "Количество слов меньше 4-х символов в корпусе sents_train = 5929 (11.05%)\n",
      "Количество слов больше 8-и символов в корпусе sents_train = 16221 (30.23%)\n",
      "--------------------------\n",
      "Средняя длина слова в корпусе sents_test = 7.3\n",
      "Количество слов меньше 4-х символов в корпусе sents_test = 1216 (10.71%)\n",
      "Количество слов больше 8-и символов в корпусе sents_test = 3584 (31.58%)\n",
      "|||||||||||||||||||||||||\n",
      "Тренировка тэггера...\n",
      "Длина суффикса  5\n",
      "Минимальная стемма  0\n",
      "Результат: 54.300000000000004 %\n",
      "-----------------------------\n",
      "Тренировка тэггера...\n",
      "Длина суффикса  5\n",
      "Минимальная стемма  1\n",
      "Результат: 46.1 %\n",
      "-----------------------------\n",
      "Тренировка тэггера...\n",
      "Длина суффикса  4\n",
      "Минимальная стемма  0\n",
      "Результат: 58.9 %\n",
      "-----------------------------\n",
      "Тренировка тэггера...\n",
      "Длина суффикса  4\n",
      "Минимальная стемма  1\n",
      "Результат: 53.6 %\n",
      "-----------------------------\n",
      "Тренировка тэггера...\n",
      "Длина суффикса  4\n",
      "Минимальная стемма  2\n",
      "Результат: 45.9 %\n",
      "-----------------------------\n",
      "Тренировка тэггера...\n",
      "Длина суффикса  3\n",
      "Минимальная стемма  0\n",
      "Результат: 57.8 %\n",
      "-----------------------------\n",
      "Тренировка тэггера...\n",
      "Длина суффикса  3\n",
      "Минимальная стемма  1\n",
      "Результат: 52.5 %\n",
      "-----------------------------\n",
      "Тренировка тэггера...\n",
      "Длина суффикса  3\n",
      "Минимальная стемма  2\n",
      "Результат: 48.4 %\n",
      "-----------------------------\n",
      "Тренировка тэггера...\n",
      "Длина суффикса  3\n",
      "Минимальная стемма  3\n",
      "Результат: 42.0 %\n",
      "-----------------------------\n",
      "Тренировка тэггера...\n",
      "Длина суффикса  2\n",
      "Минимальная стемма  0\n",
      "Результат: 42.9 %\n",
      "-----------------------------\n",
      "Тренировка тэггера...\n",
      "Длина суффикса  2\n",
      "Минимальная стемма  1\n",
      "Результат: 41.8 %\n",
      "-----------------------------\n",
      "Тренировка тэггера...\n",
      "Длина суффикса  2\n",
      "Минимальная стемма  2\n",
      "Результат: 38.5 %\n",
      "-----------------------------\n",
      "Тренировка тэггера...\n",
      "Длина суффикса  2\n",
      "Минимальная стемма  3\n",
      "Результат: 36.1 %\n",
      "-----------------------------\n"
     ]
    }
   ],
   "source": [
    "with open('media_train.tab', encoding = 'utf-8', mode='r') as f:\n",
    "    sents_train = list(read_corpus_to_nltk(f))\n",
    "\n",
    "with open('media_test.tab', encoding = 'utf-8', mode='r') as r:\n",
    "    sents_test = list(read_corpus_to_nltk(r))\n",
    "\n",
    "rmv_irr_tokens(sents_train) #Удаляем все слова, кроме {'GRND', 'ADJS', 'VERB', 'PRED', 'INFN', 'COMP', 'ADVB', 'ADJF', 'NOUN'}\n",
    "rmv_irr_tokens(sents_test)\n",
    "\n",
    "words_stats(sents_train, name = \"sents_train\") #Длины слов после чистки\n",
    "print('--------------------------')\n",
    "words_stats(sents_test, name = \"sents_test\")\n",
    "\n",
    "print(\"|||||||||||||||||||||||||\")\n",
    "go_thru_loop(sents_train, sents_test) #После удаления нерелевантных слов и знаков (результаты улучшились)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "Количество словоупотреблений до чистки: 92720\n",
      "Количество удаленных токенов: 39062\n",
      "Количество словоупотреблений после чистки: 53658\n",
      "--------------------\n",
      "--------------------\n",
      "Количество словоупотреблений до чистки: 19168\n",
      "Количество удаленных токенов: 7819\n",
      "Количество словоупотреблений после чистки: 11349\n",
      "--------------------\n",
      "--------------------\n",
      "Оставшиеся метки:  {'ADJS', 'ADJF', 'INFN', 'PRED', 'COMP', 'GRND', 'NOUN', 'ADVB', 'VERB'}\n",
      "--------------------\n",
      "Тренировка тэггера...\n",
      "Длина суффикса  5\n",
      "Минимальная стемма  0\n",
      "Результат: 70.0 %\n",
      "-----------------------------\n",
      "Тренировка тэггера...\n",
      "Длина суффикса  5\n",
      "Минимальная стемма  1\n",
      "Результат: 59.699999999999996 %\n",
      "-----------------------------\n",
      "Тренировка тэггера...\n",
      "Длина суффикса  4\n",
      "Минимальная стемма  0\n",
      "Результат: 81.5 %\n",
      "-----------------------------\n",
      "Тренировка тэггера...\n",
      "Длина суффикса  4\n",
      "Минимальная стемма  1\n",
      "Результат: 74.2 %\n",
      "-----------------------------\n",
      "Тренировка тэггера...\n",
      "Длина суффикса  4\n",
      "Минимальная стемма  2\n",
      "Результат: 63.5 %\n",
      "-----------------------------\n",
      "Тренировка тэггера...\n",
      "Длина суффикса  3\n",
      "Минимальная стемма  0\n",
      "Результат: 88.6 %\n",
      "-----------------------------\n",
      "Тренировка тэггера...\n",
      "Длина суффикса  3\n",
      "Минимальная стемма  1\n",
      "Результат: 81.3 %\n",
      "-----------------------------\n",
      "Тренировка тэггера...\n",
      "Длина суффикса  3\n",
      "Минимальная стемма  2\n",
      "Результат: 74.7 %\n",
      "-----------------------------\n",
      "Тренировка тэггера...\n",
      "Длина суффикса  3\n",
      "Минимальная стемма  3\n",
      "Результат: 64.2 %\n",
      "-----------------------------\n",
      "Тренировка тэггера...\n",
      "Длина суффикса  2\n",
      "Минимальная стемма  0\n",
      "Результат: 81.5 %\n",
      "-----------------------------\n",
      "Тренировка тэггера...\n",
      "Длина суффикса  2\n",
      "Минимальная стемма  1\n",
      "Результат: 80.10000000000001 %\n",
      "-----------------------------\n",
      "Тренировка тэггера...\n",
      "Длина суффикса  2\n",
      "Минимальная стемма  2\n",
      "Результат: 74.7 %\n",
      "-----------------------------\n",
      "Тренировка тэггера...\n",
      "Длина суффикса  2\n",
      "Минимальная стемма  3\n",
      "Результат: 69.1 %\n",
      "-----------------------------\n"
     ]
    }
   ],
   "source": [
    "with open('media_train.tab', encoding = 'utf-8', mode='r') as f:\n",
    "    sents_train = list(read_corpus_to_nltk(f))\n",
    "\n",
    "with open('media_test.tab', encoding = 'utf-8', mode='r') as r:\n",
    "    sents_test = list(read_corpus_to_nltk(r))\n",
    "\n",
    "rmv_irr_tokens(sents_train)\n",
    "rmv_irr_tokens(sents_test)\n",
    "\n",
    "full_morph_to_pos(sents_train) #Теперь тренируем POS тэггер\n",
    "full_morph_to_pos(sents_test)\n",
    "\n",
    "pos_list = []\n",
    "for sent in sents_train:\n",
    "    for word_pos in sent:\n",
    "        pos = word_pos[1]\n",
    "        pos_list.append(pos)\n",
    "print('--------------------')\n",
    "print(\"Оставшиеся метки: \", set(pos_list))\n",
    "print('--------------------')\n",
    "\n",
    "go_thru_loop(sents_train, sents_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "Количество словоупотреблений до чистки: 92720\n",
      "Количество удаленных токенов: 39062\n",
      "Количество словоупотреблений после чистки: 53658\n",
      "--------------------\n",
      "----------\n",
      "(Хурма, NOUN,femn,inan,sing,nomn), \n",
      "(термос, NOUN,masc,inan,sing,nomn), \n",
      "(глокая, ADJF,femn,sing,nomn), \n",
      "(куздра, NOUN,masc,anim,sing,gent), \n",
      "(самовар, NOUN,masc,inan,sing,accs), \n",
      "(варенька, NOUN,masc,anim,sing,nomn), \n",
      "(этот, ADJF,Apro,masc,sing,accs,inan), \n",
      "(соль, NOUN,femn,inan,sing,accs), \n",
      "----------\n",
      "Неопознанных слов: 0\n",
      "\n",
      "----------\n",
      "(Хурма, NOUN), \n",
      "(термос, NOUN), \n",
      "(глокая, ADJF), \n",
      "(куздра, NOUN), \n",
      "(самовар, NOUN), \n",
      "(варенька, NOUN), \n",
      "(этот, ADJF), \n",
      "(соль, NOUN), \n",
      "----------\n",
      "Неопознанных слов: 0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Тест на своем списке\n",
    "\n",
    "cool_list  = [\"Хурма\", \"термос\", \"глокая\", \"куздра\", \"самовар\", \"варенька\", \"этот\", \"соль\"]\n",
    "\n",
    "with open('media_train.tab', encoding = 'utf-8', mode='r') as f:\n",
    "    sents_train = list(read_corpus_to_nltk(f))\n",
    "\n",
    "#Полный морф разбор\n",
    "rmv_irr_tokens(sents_train)\n",
    "test_full_tagger = sequential.AffixTagger(train=sents_train, affix_length=-3, min_stem_length=0, verbose=False)\n",
    "apply_tagger_to_list(test_full_tagger, cool_list)\n",
    "\n",
    "#POS тэггинг\n",
    "full_morph_to_pos(sents_train)\n",
    "test_pos_tagger = sequential.AffixTagger(train=sents_train, affix_length=-3, min_stem_length=0, verbose=False)\n",
    "apply_tagger_to_list(test_pos_tagger, cool_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
